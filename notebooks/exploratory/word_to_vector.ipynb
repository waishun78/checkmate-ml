{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vector Embeddings\n",
    "\n",
    "Using Word2Vector embeddings from the word2vec-google-news-300 to find a representative word embedding of trivial and non-trivial messages (from the training sentences). Classify each new sentence based on the Euclidean distance from the representative embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec-google-news-300 model used for embeddings\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkmate_messages_df = pd.read_csv('../../src/data/CheckMate_Messages_Table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting a sentence to a 300-dimensional vector \n",
    "# Each word -> 300-dimensional vector -> average all vectors in a sentence\n",
    "\n",
    "# Note: Ignore any words not in corpus\n",
    "# Note: Return None if no matching words in sentence \n",
    "\n",
    "def sentence_2_vector(sentence):\n",
    "    message_vectors = []\n",
    "    is_na = True\n",
    "    for i in range(len(sentence)):\n",
    "        if wv.__contains__(sentence[i]):\n",
    "            message_vectors.append(wv.word_vec(sentence[i]))\n",
    "            is_na = False\n",
    "        else:\n",
    "            pass\n",
    "    if not is_na:\n",
    "        ave_vector = np.average(message_vectors, axis=0, keepdims=True)\n",
    "        return ave_vector\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepping data\n",
    "checkmate_messages_df = pd.read_csv('../../src/data/CheckMate_Messages_Table.csv')\n",
    "checkmate_messages_df.dropna()\n",
    "checkmate_messages_df['is_trivial'] = (checkmate_messages_df['taggedCategory']=='Trivial')\n",
    "# Not all sentences are imported as strings\n",
    "checkmate_messages_df['text'] = checkmate_messages_df['text'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only retrieving text and is_trivial parameters\n",
    "df = checkmate_messages_df[['text','is_trivial']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split at 50%\n",
    "df_train, df_test= train_test_split(\n",
    "        df, test_size=0.50, random_state=42, stratify=df['is_trivial'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1v/_nnjhmhj2fz8d8dg63jpvjf80000gp/T/ipykernel_25906/2803594777.py:12: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  message_vectors.append(wv.word_vec(sentence[i]))\n"
     ]
    }
   ],
   "source": [
    "# Creating representative vector embeddings for is_trivial and not_trivial\n",
    "df_train_is_trivial = df_train[df_train['is_trivial']==True]\n",
    "df_train_not_trivial = df_train[df_train['is_trivial']==False]\n",
    "\n",
    "vectors_train_is_trivial = df_train_is_trivial['text'].map(sentence_2_vector).dropna()\n",
    "vectors_train_not_trivial = df_train_not_trivial['text'].map(sentence_2_vector).dropna()\n",
    "\n",
    "# df_train_is_trivial.dropna(inplace=True)\n",
    "# df_train_not_trivial.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_vector_is_trivial = np.average(vectors_train_is_trivial)\n",
    "ave_vector_not_trivial = np.average(vectors_train_not_trivial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the sentences based on euclidean distances from the representative vectors\n",
    "def classify_is_trivial(v):\n",
    "    d_is_trivial = np.linalg.norm(ave_vector_is_trivial - v)\n",
    "    d_not_trivial = np.linalg.norm(ave_vector_not_trivial - v)\n",
    "    # print(f'distance from trivial:{d_is_trivial}, not trivial:{d_not_trivial}')\n",
    "    if d_is_trivial<= d_not_trivial:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77    [[-0.12804633, 0.07846623, -0.0028197807, 0.12...\n",
      "31    [[0.0596852, -0.13528802, 0.097473145, -0.0677...\n",
      "61    [[-0.19146292, 0.111301966, 0.002633231, 0.150...\n",
      "67    [[-0.15674461, 0.10766732, -0.0025885978, 0.13...\n",
      "12    [[-0.17181522, 0.1035043, -0.0022740315, 0.133...\n",
      "16    [[-0.1640625, 0.09762974, -0.023943044, 0.1348...\n",
      "30    [[-0.11051596, 0.062171426, 0.02830754, 0.1142...\n",
      "58    [[-0.16927543, 0.061089292, 0.0279886, 0.13594...\n",
      "13    [[-0.17247473, 0.113895744, 0.0021641373, 0.12...\n",
      "43    [[-0.25672743, 0.13237847, -0.003458659, 0.140...\n",
      "17    [[-0.18310767, 0.080474116, 0.004465126, 0.141...\n",
      "38    [[-0.14813232, 0.055023193, -0.03371175, 0.152...\n",
      "32    [[-0.16450882, 0.095428996, 0.014264863, 0.129...\n",
      "40    [[-0.21744792, 0.12727864, 0.009562175, 0.1368...\n",
      "66    [[-0.16306363, 0.088267356, 0.013311978, 0.138...\n",
      "68    [[-0.14635438, 0.09423035, 0.022300415, 0.1275...\n",
      "44    [[-0.15293118, 0.09489967, 0.024162214, 0.1230...\n",
      "27    [[-0.16092487, 0.10481708, 0.0009290307, 0.122...\n",
      "48    [[-0.013977051, -0.02722168, -0.05493164, 0.17...\n",
      "70    [[-0.11381836, 0.14484863, -0.019433593, 0.023...\n",
      "26    [[-0.17764743, 0.11487151, -0.0037715703, 0.13...\n",
      "1     [[-0.17118599, 0.104189046, -0.017225478, 0.14...\n",
      "41    [[-0.19171482, 0.10688782, 0.008826362, 0.1218...\n",
      "23    [[-0.2044781, 0.12843862, 0.0038480319, 0.1225...\n",
      "2     [[-0.11132282, 0.0926695, -0.0031623289, 0.102...\n",
      "53    [[-0.15271714, 0.091858916, 0.0064919214, 0.10...\n",
      "35    [[-0.16696043, 0.07858689, 0.030725427, 0.1289...\n",
      "76    [[-0.16709909, 0.12116419, 0.0016416807, 0.130...\n",
      "63    [[-0.14298055, 0.1146442, 0.027101679, 0.12118...\n",
      "8     [[-0.013977051, -0.02722168, -0.05493164, 0.17...\n",
      "55    [[-0.14506404, 0.08306631, 0.011984762, 0.1207...\n",
      "72    [[-0.18551181, 0.11282576, 0.00046945125, 0.14...\n",
      "29    [[-0.16509782, 0.11001235, 0.0076972363, 0.132...\n",
      "24    [[-0.10410945, 0.058006424, 0.018029228, 0.102...\n",
      "62    [[-0.23976956, 0.11821571, -0.02938608, 0.1302...\n",
      "18    [[-0.14650655, 0.08997771, 0.022587573, 0.1214...\n",
      "69    [[-0.25341797, 0.004211426, 0.0569458, 0.10308...\n",
      "51    [[-0.15909529, 0.1054484, 0.010236575, 0.12862...\n",
      "25    [[-0.09518827, 0.054578718, 0.050228488, 0.119...\n",
      "11    [[-0.18754783, 0.13506255, -0.037604254, 0.124...\n",
      "46    [[-0.12065506, 0.10536194, -0.014846802, 0.139...\n",
      "85    [[-0.17479755, 0.115916066, 0.0036882672, 0.12...\n",
      "21    [[-0.101107664, 0.078763776, 0.061069436, 0.10...\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1v/_nnjhmhj2fz8d8dg63jpvjf80000gp/T/ipykernel_25906/2803594777.py:12: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  message_vectors.append(wv.word_vec(sentence[i]))\n"
     ]
    }
   ],
   "source": [
    "# Create a series containing the average vector represenations of each sentence\n",
    "vectors_test = df_test['text'].map(sentence_2_vector).dropna()\n",
    "print(vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77    False\n",
      "31    False\n",
      "61    False\n",
      "67    False\n",
      "12    False\n",
      "16    False\n",
      "30    False\n",
      "58    False\n",
      "13    False\n",
      "43    False\n",
      "17    False\n",
      "38    False\n",
      "32    False\n",
      "40    False\n",
      "66    False\n",
      "68    False\n",
      "44    False\n",
      "27    False\n",
      "48    False\n",
      "70     True\n",
      "26    False\n",
      "1     False\n",
      "41     True\n",
      "23    False\n",
      "2     False\n",
      "53    False\n",
      "35    False\n",
      "76    False\n",
      "63    False\n",
      "8     False\n",
      "55    False\n",
      "72    False\n",
      "29    False\n",
      "24    False\n",
      "62    False\n",
      "18    False\n",
      "69     True\n",
      "51    False\n",
      "25    False\n",
      "11    False\n",
      "46     True\n",
      "85    False\n",
      "21    False\n",
      "Name: text, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# Create prediction for each sentence in series \n",
    "prediction = (vectors_test.map(classify_is_trivial))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  target  prediction\n",
      "77  Fast Cash Service                \\n3k 268 x12m...   False       False\n",
      "31  中国确实真的有5千年历史啊！这是对的啊！谁人的屁是香的啊！老外的屁也是臭味熏天的啊！他们还有...   False       False\n",
      "61                               can u read pictures?    True       False\n",
      "67  Hello, We have noticed your employment history...   False       False\n",
      "12  Hey Jolyn! We haven't seen you in the studio y...   False       False\n",
      "16  So...... Starts walking!!!\\n\\nhttps://www.thev...   False       False\n",
      "30  Date: 24 March 2023 at 2:24:03 pm AEDT\\nTo: \\n...   False       False\n",
      "58  Hi good evening. How u get my Number ? My side...   False       False\n",
      "13  Hi all, especially for our younger colleagues ...   False       False\n",
      "43                                     is this a scam    True       False\n",
      "17  Suck lozenges n  rub tiger balm on nose , last...    True       False\n",
      "38                             9/11 attack was a scam   False       False\n",
      "32  <ADV> Singapore's medical cost is always risin...   False       False\n",
      "40                                              scam?    True       False\n",
      "66  Hi there, lovely Evening, we are currently lau...   False       False\n",
      "68  Dear customer, Your shipment No. FR10371889SG ...   False       False\n",
      "44  Hi , this is Julia from Chan Brothers Travel P...   False       False\n",
      "27  LIMITED TIME till end of this month!Save up to...   False       False\n",
      "48                                                nan   False       False\n",
      "70                                              Hello    True        True\n",
      "26  This is one of the most profound description o...   False       False\n",
      "1   https://www.mas.gov.sg/news/media-releases/202...   False       False\n",
      "41  Hello \\nsorry to bother you, is this Kevin Wong ?   False        True\n",
      "23  Doctors Honoured by Hitler https://youtube.com...   False       False\n",
      "2   TN 95546718362782 is out for del. Allow for co...   False       False\n",
      "53  T2Club Double Bonus For New Members Up To $300...   False       False\n",
      "35  <ADV> Congrats Ming! You're one of our 3 lucky...   False       False\n",
      "76  FYI, please. Please kindly adhere to the ETA (...    True       False\n",
      "63  Yes should be OK\\nLet me check n confirm\\nI th...    True       False\n",
      "8                                                 nan   False       False\n",
      "55  World Cup Coming\\nCome open your Acct\\n*5Years...   False       False\n",
      "72  If you receive a scam message like this simula...    True       False\n",
      "29  News from Singapore!\\nPlease be sure to wear a...   False       False\n",
      "24  [8/04, today's Zaobao Chinese Morning newspape...   False       False\n",
      "62                                    Am just testing    True       False\n",
      "18  📢 HUUUURRY! Use your code GLASSSKIN10 & start ...   False       False\n",
      "69                                                Hi!    True        True\n",
      "51  Hi, I'm. Felicia.  from *SG, Employment Agency...   False       False\n",
      "25  https://m.facebook.com/story.php/?story_fbid=p...   False       False\n",
      "11  Hello, no matter how busy work remember to eat Oh    True       False\n",
      "46                            Well done CheckMate 👏😅👏    True        True\n",
      "85  Excuse me, this is Stella, have you arranged a...   False       False\n",
      "21  Tracking No QXSGD01808049 has been successfull...   False       False\n"
     ]
    }
   ],
   "source": [
    "# Concat the prediction and true values for the comparison\n",
    "predicted_vs_output = pd.concat([df_test['text'], df_test['is_trivial'], prediction], axis=1)\n",
    "# predicted_vs_output = predicted_vs_output.rename(columns={'is_trivial':'target', 'text':'prediction'})\n",
    "predicted_vs_output.columns = ['text', 'target','prediction']\n",
    "\n",
    "print(predicted_vs_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  target  prediction\n",
      "41  Hello \\nsorry to bother you, is this Kevin Wong ?   False        True\n"
     ]
    }
   ],
   "source": [
    "# print(predicted_vs_output)\n",
    "print(predicted_vs_output[(predicted_vs_output['target']==False) & (predicted_vs_output['prediction']==True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the recall of this methodology\n",
    "true_positive = len(predicted_vs_output[(predicted_vs_output['target']==True) & (predicted_vs_output['prediction']==True)])\n",
    "true_negative = len(predicted_vs_output[(predicted_vs_output['target']==False) & (predicted_vs_output['prediction']==False)])\n",
    "\n",
    "false_negative = len(predicted_vs_output[(predicted_vs_output['target']==True) & (predicted_vs_output['prediction']==False)])\n",
    "false_positive = len(predicted_vs_output[(predicted_vs_output['target']==False) & (predicted_vs_output['prediction']==True)])\n",
    "\n",
    "recall = true_positive/(true_positive+false_negative)\n",
    "accuracy = (true_positive+true_negative)/(true_positive+true_negative+false_positive+false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.7674418604651163\n",
      "3\n",
      "30\n",
      "9\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(recall)\n",
    "print(accuracy)\n",
    "\n",
    "print(true_positive)\n",
    "print(true_negative)\n",
    "print(false_negative)\n",
    "print(false_positive)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "Recall = 66.6% \n",
    "Accuracy = 87.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextualised recall:0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "# However in the context of this case we care about how many were classified correct over all classified correctly + those we flag as trivial but actually not trivial\n",
    "\n",
    "recall_contextualised = true_negative/(true_negative+false_positive)\n",
    "print(f'Contextualised recall:{recall_contextualised}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Room for improvement:\n",
    "1. Tokens are generated by a simple split(,) function. Using a tokeniser trained on the english might produce better results\n",
    "2. Using a encoder like bert trained on a larger corpus could give better results than word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checkmate_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brainstorm \n",
    "Understanding the Hugging Face tokenizer object and how to encode sentences using them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "hi, how are you?\n"
     ]
    }
   ],
   "source": [
    "embeddings = tokenizer.encode('Hi, how are you?')\n",
    "print(embeddings)\n",
    "print(tokenizer.decode(embeddings.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7632, 1010, 2129, 2024, 2017, 1029, 102]\n",
      "[101, 2129, 2024, 2017, 1029, 7632, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "embeddings = tokenizer.encode('Hi, how are you?')\n",
    "embeddings2 = tokenizer.encode('How are you? Hi. ')\n",
    "print(embeddings.ids)\n",
    "print(embeddings2.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi.\n",
      "hi +\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(embeddings.ids[0:2]))\n",
    "print(tokenizer.decode(embeddings2.ids[-3:-1]))\n",
    "print(tokenizer.decode([101, 7632, 1009]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi,\n",
      "? hi.\n",
      "hi -\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(embeddings.ids[0:3]))\n",
    "print(tokenizer.decode(embeddings2.ids[-4:-1]))\n",
    "print(tokenizer.decode([101, 7632, 1011]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2572, 5206, 102]\n",
      "[101, 1045, 2572, 5458, 102]\n"
     ]
    }
   ],
   "source": [
    "embeddings = tokenizer.encode('I am johnny')\n",
    "embeddings2 = tokenizer.encode('I am tired')\n",
    "print(embeddings.ids)\n",
    "print(embeddings2.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "johnny\n",
      "tired\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(embeddings.ids[-2:-1]))\n",
    "print(tokenizer.decode(embeddings2.ids[-2:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3035, 102]\n",
      "[101, 4615, 102]\n",
      "[101, 2332, 102]\n",
      "[101, 3159, 102]\n"
     ]
    }
   ],
   "source": [
    "embeddings = tokenizer.encode('queen')\n",
    "embeddings2 = tokenizer.encode('princess')\n",
    "print(embeddings.ids)\n",
    "print(embeddings2.ids)\n",
    "\n",
    "embeddings3 = tokenizer.encode('king')\n",
    "embeddings4 = tokenizer.encode('prince')\n",
    "print(embeddings3.ids)\n",
    "print(embeddings4.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([3035]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0993a62dd04dfd89de44086240b8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa09823c96b461fa0da168a7680eba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29cd1f9facd241d28d59796194e710dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb4185ea543412ba5795555c7055fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# load\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode('king', add_special_tokens=True)\n",
    "input_ids2 = tokenizer.encode('king arthur', add_special_tokens=True)\n",
    "input_ids3 = tokenizer.encode('king kong', add_special_tokens=True)\n",
    "input_ids4 = tokenizer.encode('arthur', add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2332, 102]\n",
      "[101, 2332, 4300, 102]\n",
      "[101, 2332, 4290, 102]\n",
      "[101, 4300, 102]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n",
    "print(input_ids2)\n",
    "print(input_ids3)\n",
    "print(input_ids4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Here is some text to encode\"\n",
    "# tokenizer-> token_id\n",
    "input_ids = tokenizer.encode(input_text, add_special_tokens=True)\n",
    "# input_ids: [101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102]\n",
    "input_ids = torch.tensor([input_ids])\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)[0] # Models outputs are now tuples\n",
    "last_hidden_states = last_hidden_states.mean(1)\n",
    "print(last_hidden_states)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Word Embeddings Tutorial \n",
    "From https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
    "       \"fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2044, 11065,  2769,  2013,  1996,  2924, 11632,  1010,  1996,\n",
      "          2924, 27307,  2001,  2464,  5645,  2006,  1996,  5900,  2314,  2924,\n",
      "          1012,   102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(tokens_tensor)\n",
    "print(segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>\n",
      "(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [ 0.2329,  0.1390,  0.2979,  ..., -0.0655,  0.8885,  0.5109],\n",
      "         [ 0.2257, -0.7165, -0.7255,  ...,  0.4844,  0.6030, -0.0957],\n",
      "         ...,\n",
      "         [-0.0374, -0.6155, -1.4419,  ...,  0.0793, -0.0811, -0.3802],\n",
      "         [-0.0228,  0.4207, -0.3288,  ...,  0.4464,  0.5178,  0.5501],\n",
      "         [-0.2350,  0.1566, -0.0462,  ..., -0.4206,  0.3074, -0.2288]]]), tensor([[[ 0.0522,  0.0595, -0.2179,  ...,  0.2280, -0.0712,  0.0148],\n",
      "         [ 0.3819,  0.1475,  0.2414,  ...,  0.3397,  0.7607,  0.4999],\n",
      "         [ 0.1705, -0.6168, -0.7296,  ...,  0.8631,  0.6274, -0.3727],\n",
      "         ...,\n",
      "         [ 0.6982, -0.4554, -1.7845,  ...,  0.3308,  0.0710, -0.5187],\n",
      "         [-0.0905,  0.1862, -0.4437,  ...,  0.2244,  0.1810,  0.3740],\n",
      "         [-0.0825,  0.0466, -0.1526,  ..., -0.2033,  0.3370, -0.1767]]]), tensor([[[-0.0357, -0.2022, -0.4103,  ...,  0.2511,  0.0586, -0.0547],\n",
      "         [ 0.1430,  0.0747,  0.0595,  ..., -0.2914,  0.1733,  0.4265],\n",
      "         [ 0.5752, -1.2385, -0.5650,  ...,  0.8995,  0.4652, -0.8080],\n",
      "         ...,\n",
      "         [ 0.9157, -0.4118, -1.6042,  ...,  0.1454,  0.1699, -0.2230],\n",
      "         [-0.1816,  0.0099,  0.0370,  ..., -0.2178,  0.0481,  0.3477],\n",
      "         [-0.1524, -0.1068, -0.0868,  ..., -0.1369,  0.2633, -0.3754]]]), tensor([[[-0.0183, -0.3853, -0.1600,  ...,  0.2446,  0.2160,  0.0633],\n",
      "         [ 0.7727, -0.3786,  0.6677,  ..., -0.2014,  0.0094,  0.6461],\n",
      "         [ 0.8822, -0.7807, -0.6305,  ...,  0.5223,  0.3687, -0.9875],\n",
      "         ...,\n",
      "         [ 0.9788, -0.0669, -1.6344,  ...,  0.0341,  0.0437, -0.1636],\n",
      "         [-0.4724, -0.0258,  0.3107,  ..., -0.2553, -0.1049,  0.0897],\n",
      "         [-0.0816, -0.1028,  0.0947,  ...,  0.0091,  0.0649, -0.0646]]]), tensor([[[ 0.0207, -0.3990, -0.8255,  ...,  0.3448,  0.0951,  0.3650],\n",
      "         [ 0.5667, -0.7513,  0.0695,  ..., -0.1052, -0.1700,  0.4333],\n",
      "         [ 0.7095, -0.2044, -0.1709,  ...,  0.7092,  0.0542, -1.0380],\n",
      "         ...,\n",
      "         [ 1.3519,  0.3352, -1.3555,  ...,  0.1676, -0.2817,  0.1042],\n",
      "         [-0.2488,  0.1044, -0.1778,  ..., -0.2713, -0.2643,  0.2310],\n",
      "         [-0.0266, -0.0469,  0.0053,  ...,  0.0085,  0.0417, -0.0486]]]), tensor([[[-0.3443, -0.6315, -0.6647,  ...,  0.0050,  0.1167,  0.3614],\n",
      "         [ 0.8173, -0.9999, -0.1752,  ..., -0.4932, -0.2556,  0.2038],\n",
      "         [ 0.8973, -0.2652, -0.6559,  ...,  0.5725,  0.4178, -0.6356],\n",
      "         ...,\n",
      "         [ 1.5582,  0.4479, -1.2054,  ...,  0.2052, -0.5083,  0.3994],\n",
      "         [-0.1898,  0.0898, -0.2766,  ..., -0.3044, -0.4370,  0.4248],\n",
      "         [-0.0264, -0.0364,  0.0304,  ...,  0.0160,  0.0067, -0.0486]]]), tensor([[[-2.3737e-01, -9.9507e-01, -4.5364e-01,  ..., -2.1456e-01,\n",
      "           3.5440e-01,  2.5447e-01],\n",
      "         [ 5.7495e-01, -9.1262e-01, -2.3175e-01,  ..., -1.8444e-01,\n",
      "          -1.8389e-01,  1.3615e-01],\n",
      "         [ 2.5019e-01, -5.6334e-01, -7.0052e-01,  ...,  2.0830e-01,\n",
      "           6.6902e-01,  5.5973e-02],\n",
      "         ...,\n",
      "         [ 1.5415e+00,  3.5883e-03, -1.0563e+00,  ...,  6.0116e-02,\n",
      "          -3.6085e-01,  5.3854e-01],\n",
      "         [-5.8238e-01,  8.6807e-02, -4.4414e-01,  ..., -8.1630e-01,\n",
      "          -4.2633e-01,  4.1039e-01],\n",
      "         [ 1.3974e-02, -5.6850e-02, -4.0642e-03,  ...,  7.8809e-05,\n",
      "          -1.4510e-02, -4.7457e-02]]]), tensor([[[-1.5213e-01, -9.7455e-01, -5.6574e-01,  ..., -6.0024e-01,\n",
      "           3.0606e-01,  3.6930e-01],\n",
      "         [ 2.7012e-01, -5.4540e-01,  1.6653e-02,  ...,  6.2675e-02,\n",
      "           2.6824e-01,  3.4129e-02],\n",
      "         [-4.1255e-01, -6.4567e-01, -2.0088e-01,  ...,  2.3458e-01,\n",
      "           1.4713e-01,  4.0928e-03],\n",
      "         ...,\n",
      "         [ 1.3217e+00, -6.6626e-02, -8.1355e-01,  ..., -3.9645e-01,\n",
      "          -5.2719e-01,  2.0277e-01],\n",
      "         [-5.6073e-01, -4.5253e-01, -5.8201e-01,  ..., -1.4335e+00,\n",
      "           2.8755e-01,  5.9470e-01],\n",
      "         [ 6.2766e-05, -2.2453e-02, -2.1868e-02,  ..., -1.4897e-02,\n",
      "           2.8921e-03, -4.3436e-02]]]), tensor([[[ 0.0967, -0.8567, -0.5056,  ..., -0.4204,  0.1267,  0.3225],\n",
      "         [-0.0109, -0.2976,  0.2986,  ..., -0.3344,  0.2755, -0.1571],\n",
      "         [-0.6171, -0.4909, -0.1630,  ..., -0.3340,  0.4452, -0.2641],\n",
      "         ...,\n",
      "         [ 0.9738,  0.0119, -0.6562,  ..., -0.3114, -0.1033,  0.1980],\n",
      "         [-0.8265, -0.4334, -0.7586,  ..., -1.2721,  0.0482,  0.3751],\n",
      "         [ 0.0196,  0.0017,  0.0323,  ..., -0.0348, -0.0410, -0.0743]]]), tensor([[[-0.3495, -0.7788, -0.7701,  ..., -0.0715,  0.2350,  0.1686],\n",
      "         [-0.2694, -0.3884, -0.3181,  ..., -0.7334,  0.0196, -0.4148],\n",
      "         [-0.5444, -0.3546, -0.2131,  ..., -0.5306,  0.2799, -0.3587],\n",
      "         ...,\n",
      "         [ 0.6356,  0.0980, -0.2775,  ..., -0.6243, -0.2620,  0.1343],\n",
      "         [-0.4167,  0.1132, -0.4761,  ..., -0.5415, -0.1268,  0.0483],\n",
      "         [-0.0671, -0.0451,  0.0232,  ..., -0.0718, -0.0020, -0.0556]]]), tensor([[[-0.9560, -1.0372, -0.8875,  ...,  0.0905, -0.3867,  0.3258],\n",
      "         [-0.3215, -1.1213, -0.2349,  ..., -0.5940,  0.1851, -0.5076],\n",
      "         [-0.4795, -1.0745, -0.1744,  ..., -0.5026,  0.1293, -0.1963],\n",
      "         ...,\n",
      "         [ 0.2736, -0.4021, -0.1525,  ..., -0.7099, -0.7498, -0.0066],\n",
      "         [-0.0143,  0.0187, -0.0490,  ..., -0.0197, -0.0281,  0.0165],\n",
      "         [-0.5765, -0.5547, -0.2122,  ...,  0.2610, -0.4111, -0.1223]]]), tensor([[[-0.6121, -0.6371, -0.8917,  ...,  0.1026, -0.2241,  0.3330],\n",
      "         [-0.2817, -0.6142, -0.4499,  ..., -0.7273,  0.0304, -0.5106],\n",
      "         [-0.4519, -0.2628, -0.1917,  ..., -0.5405, -0.2146, -0.2140],\n",
      "         ...,\n",
      "         [ 0.3213, -0.2997, -0.0471,  ..., -0.8094, -0.7861, -0.0618],\n",
      "         [ 0.0430,  0.0219, -0.0214,  ...,  0.0196, -0.0353,  0.0072],\n",
      "         [-0.0599, -0.6397, -0.6008,  ...,  0.4218, -0.5170, -0.1616]]]), tensor([[[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
      "         [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
      "         [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
      "         ...,\n",
      "         [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
      "         [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n",
      "         [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]]]))\n",
      "13\n",
      "1\n",
      "22\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(type(outputs))\n",
    "print(outputs.hidden_states)\n",
    "print(len(outputs.hidden_states))# Model has 13 layers\n",
    "print(len(outputs.hidden_states[0])) # Model has one batch\n",
    "print(len(outputs.hidden_states[0][0])) # Model took 22 tokens in the sentence\n",
    "print(len(outputs.hidden_states[0][0][0])) # Model has 768 features in its hidden unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 22, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "# Converting a basemodeloutput object into a tensor object\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
      "            3.8253e-02,  1.6400e-01],\n",
      "          [ 2.3295e-01,  1.3898e-01,  2.9788e-01,  ..., -6.5465e-02,\n",
      "            8.8849e-01,  5.1089e-01],\n",
      "          [ 2.2572e-01, -7.1647e-01, -7.2547e-01,  ...,  4.8439e-01,\n",
      "            6.0302e-01, -9.5701e-02],\n",
      "          ...,\n",
      "          [-3.7402e-02, -6.1545e-01, -1.4419e+00,  ...,  7.9256e-02,\n",
      "           -8.1097e-02, -3.8018e-01],\n",
      "          [-2.2755e-02,  4.2067e-01, -3.2878e-01,  ...,  4.4641e-01,\n",
      "            5.1775e-01,  5.5010e-01],\n",
      "          [-2.3496e-01,  1.5656e-01, -4.6245e-02,  ..., -4.2065e-01,\n",
      "            3.0737e-01, -2.2883e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 5.2195e-02,  5.9528e-02, -2.1788e-01,  ...,  2.2799e-01,\n",
      "           -7.1235e-02,  1.4849e-02],\n",
      "          [ 3.8188e-01,  1.4754e-01,  2.4141e-01,  ...,  3.3967e-01,\n",
      "            7.6073e-01,  4.9991e-01],\n",
      "          [ 1.7047e-01, -6.1683e-01, -7.2964e-01,  ...,  8.6309e-01,\n",
      "            6.2739e-01, -3.7271e-01],\n",
      "          ...,\n",
      "          [ 6.9817e-01, -4.5541e-01, -1.7845e+00,  ...,  3.3082e-01,\n",
      "            7.0954e-02, -5.1872e-01],\n",
      "          [-9.0504e-02,  1.8623e-01, -4.4370e-01,  ...,  2.2435e-01,\n",
      "            1.8097e-01,  3.7402e-01],\n",
      "          [-8.2526e-02,  4.6579e-02, -1.5260e-01,  ..., -2.0332e-01,\n",
      "            3.3697e-01, -1.7667e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.5666e-02, -2.0223e-01, -4.1032e-01,  ...,  2.5113e-01,\n",
      "            5.8552e-02, -5.4653e-02],\n",
      "          [ 1.4299e-01,  7.4720e-02,  5.9476e-02,  ..., -2.9141e-01,\n",
      "            1.7326e-01,  4.2652e-01],\n",
      "          [ 5.7522e-01, -1.2385e+00, -5.6504e-01,  ...,  8.9952e-01,\n",
      "            4.6518e-01, -8.0804e-01],\n",
      "          ...,\n",
      "          [ 9.1570e-01, -4.1177e-01, -1.6042e+00,  ...,  1.4539e-01,\n",
      "            1.6985e-01, -2.2298e-01],\n",
      "          [-1.8156e-01,  9.9375e-03,  3.6962e-02,  ..., -2.1779e-01,\n",
      "            4.8058e-02,  3.4768e-01],\n",
      "          [-1.5242e-01, -1.0677e-01, -8.6771e-02,  ..., -1.3689e-01,\n",
      "            2.6327e-01, -3.7541e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-9.5604e-01, -1.0372e+00, -8.8754e-01,  ...,  9.0501e-02,\n",
      "           -3.8670e-01,  3.2580e-01],\n",
      "          [-3.2149e-01, -1.1213e+00, -2.3492e-01,  ..., -5.9404e-01,\n",
      "            1.8506e-01, -5.0758e-01],\n",
      "          [-4.7948e-01, -1.0745e+00, -1.7443e-01,  ..., -5.0257e-01,\n",
      "            1.2932e-01, -1.9628e-01],\n",
      "          ...,\n",
      "          [ 2.7364e-01, -4.0209e-01, -1.5246e-01,  ..., -7.0990e-01,\n",
      "           -7.4977e-01, -6.6027e-03],\n",
      "          [-1.4250e-02,  1.8744e-02, -4.9021e-02,  ..., -1.9688e-02,\n",
      "           -2.8110e-02,  1.6460e-02],\n",
      "          [-5.7646e-01, -5.5467e-01, -2.1221e-01,  ...,  2.6103e-01,\n",
      "           -4.1106e-01, -1.2234e-01]]],\n",
      "\n",
      "\n",
      "        [[[-6.1212e-01, -6.3712e-01, -8.9171e-01,  ...,  1.0260e-01,\n",
      "           -2.2410e-01,  3.3305e-01],\n",
      "          [-2.8169e-01, -6.1423e-01, -4.4993e-01,  ..., -7.2730e-01,\n",
      "            3.0355e-02, -5.1065e-01],\n",
      "          [-4.5192e-01, -2.6275e-01, -1.9171e-01,  ..., -5.4054e-01,\n",
      "           -2.1462e-01, -2.1395e-01],\n",
      "          ...,\n",
      "          [ 3.2134e-01, -2.9974e-01, -4.7083e-02,  ..., -8.0937e-01,\n",
      "           -7.8612e-01, -6.1832e-02],\n",
      "          [ 4.3049e-02,  2.1942e-02, -2.1436e-02,  ...,  1.9552e-02,\n",
      "           -3.5335e-02,  7.2243e-03],\n",
      "          [-5.9900e-02, -6.3968e-01, -6.0083e-01,  ...,  4.2177e-01,\n",
      "           -5.1704e-01, -1.6159e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.9644e-01, -1.8308e-01, -5.2315e-01,  ..., -1.9021e-01,\n",
      "            3.7380e-01,  3.9644e-01],\n",
      "          [-1.3227e-01, -2.7622e-01, -3.4954e-01,  ..., -4.5666e-01,\n",
      "            3.7865e-01, -1.0961e-01],\n",
      "          [-3.6261e-01, -4.0016e-01,  6.7574e-02,  ..., -3.2071e-01,\n",
      "           -2.7090e-01, -3.0043e-01],\n",
      "          ...,\n",
      "          [ 2.9609e-01, -2.8563e-01, -3.8183e-02,  ..., -6.0557e-01,\n",
      "           -5.1633e-01,  2.0051e-01],\n",
      "          [ 4.8781e-01, -9.0929e-02, -2.3581e-01,  ..., -1.7212e-03,\n",
      "           -5.9445e-01, -2.4313e-01],\n",
      "          [-2.5167e-01, -3.5192e-01, -4.6880e-01,  ...,  2.5005e-01,\n",
      "            3.3593e-02, -2.6271e-01]]]])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(token_embeddings)\n",
    "print(type(token_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 22, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = torch.squeeze(token_embeddings) # Remove any dimension with just size = 1\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 13, 768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many different ways to get the vector representation of the sentence vector - we have 22 x 13 x 768 vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 22 x 768\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 22, 768])\n",
      "torch.Size([22, 768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "print((hidden_states[-2].shape)) # Getting the second last layer [22x768]\n",
    "print((hidden_states[-2][0].shape)) # Getting the second last layer [22x768] ignoring he batch layer\n",
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "print(sentence_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2., 1., 1.],\n",
      "         [3., 2., 2.]],\n",
      "\n",
      "        [[4., 3., 3.],\n",
      "         [5., 4., 4.]],\n",
      "\n",
      "        [[6., 5., 5.],\n",
      "         [7., 6., 6.]]])\n",
      "torch.Size([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# t1 = torch.FloatTensor([\n",
    "#     [[2,1,1],[3,2,2]],\n",
    "#     [[4,3,3],[5,4,4]],\n",
    "#     [[6,5,5],[7,6,6]]\n",
    "# ])\n",
    "# print(t1)\n",
    "# print(t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2., 4., 6.],\n",
      "         [3., 5., 7.]],\n",
      "\n",
      "        [[1., 3., 5.],\n",
      "         [2., 4., 6.]],\n",
      "\n",
      "        [[1., 3., 5.],\n",
      "         [2., 4., 6.]]])\n"
     ]
    }
   ],
   "source": [
    "# t1 = t1.permute(2,1,0)\n",
    "# print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.94\n",
      "Vector similarity for *different* meanings:  0.69\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checkmate_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
